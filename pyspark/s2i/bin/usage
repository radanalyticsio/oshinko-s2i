#!/bin/bash -e
cat <<EOF
This is the radanalytics-pyspark S2I builder image. It will build
a radanalytics pyspark application image based on a github repository
that you specify and the resulting image may be used to run
your pyspark application.

To use it, first install S2I: https://github.com/openshift/source-to-image

Usage: s2i build [-e APP_FILE=filename] <github repo> radanalytics-pyspark <application image name>

** Important! ** The APP_FILE environment variable specifies the name of the python file that
contains the main routine for your pyspark application. The default value is
"app.py". If your main routine is in a different file, you must use the APP_FILE
option to s2i build to specify it.

Example:

    $s2i build -e APP_FILE=my_main.py git://github.com/mygithub/wordcount.git radanalytics-pyspark mynewapp

The resulting application image contains a startup script that expects the following required and
optional environment variables which can be set when the application is launched:

    OSHINKO_CLUSTER_NAME -- name of the spark cluster to use or create, required
    OSHINKO_REST         -- ip of the oshinko-rest controller, required
    SPARK_OPTIONS        -- options to spark-submit, e.g.  "--conf spark.executor.memory=2G --conf spark.driver.memory=2G"
    APP_ARGS             -- arguments to the python application (if the app takes arguments)
    OSHINKO_DEL_CLUSTER  -- if a new cluster is created and this flag is set to "true" then the
                            cluster will be deleted when the python application completes. The
                            default behavior is to leave the cluster.
    APP_EXIT             -- if this value is unset or set to "false" then the script will
                            loop forever after the spark application completes. To have the
                            script exit, set this value to "true"
EOF
