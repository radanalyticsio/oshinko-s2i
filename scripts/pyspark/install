#!/bin/sh

set -e

SCRIPT_DIR=$(dirname $0)
ADDED_DIR=${SCRIPT_DIR}/added
ARTIFACTS_DIR=/tmp/artifacts

# If there is a zero-length spark tarball, find the verison in the
# name and download from github
fullname=$(find $ARTIFACTS_DIR -name spark-[0-9.]*-bin-hadoop[0-9.]*\.tgz)
spark=$(basename $fullname)
if ! [ -s "$fullname" ]; then
    version=$(echo $spark | cut -d '-' -f2)
    wget https://archive.apache.org/dist/spark/spark-$version/$spark -O $fullname
fi
cp $fullname /opt

pushd /opt
tar -zxf $spark
ln -s $(basename $spark .tgz) spark
rm $spark
popd

set +e
pip show numpy
res=$?
set -e
if [ "$res" -ne 0 ]
then
    if [ -f /opt/app-root/etc/scl_enable ]
    then
        source /opt/app-root/etc/scl_enable && pip install numpy
    else
        pip install numpy
    fi
fi

mkdir -p /usr/libexec/s2i
cp -r $ADDED_DIR/s2i/* /usr/libexec/s2i
cp $ADDED_DIR/spark-conf/* /opt/spark/conf/
chown -R 185:0 -R $APP_ROOT && chmod a+rwX -R $APP_ROOT
chown -R 185:0 /opt/spark/conf && chmod g+rw -R /opt/spark/conf
